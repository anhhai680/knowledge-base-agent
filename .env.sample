# Knowledge Base Agent Configuration
# Copy this to .env and modify values according to your setup

# LLM Configuration
LLM_PROVIDER=ollama  # Options: openai, gemini, azure_openai, ollama
LLM_MODEL=llama3.1:8b  # Change to your preferred model
LLM_API_BASE_URL=http://localhost:11434/v1  # Change if using a different provider

# Embedding Configuration
EMBEDDING_MODEL=nomic-embed-text  # Change to your preferred embedding model
EMBEDDING_API_BASE_URL=http://localhost:11434/v1/embeddings  # Change if using a different provider

# API Keys
# Replace with your actual API keys for the respective providers
# Only configure the keys for the providers you plan to use
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
AZURE_OPENAI_API_KEY=your_azure_openai_key
AZURE_OPENAI_ENDPOINT=your_azure_endpoint

# Vector Database Configuration
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION_NAME=knowledge-base-collection

# PostgreSQL Configuration (for pgvector)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=knowledge_base
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password

# GitHub Configuration
GITHUB_TOKEN=your_github_token_here
GITHUB_REPOS=["https://github.com/user/repo1", "https://github.com/user/repo2"]

# Application Configuration
APP_ENV=development
LOG_LEVEL=INFO
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_TOKENS=4000
TEMPERATURE=0.7

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Common Model Examples:
# 
# For OpenAI:
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# EMBEDDING_MODEL=text-embedding-3-small
# 
# For Gemini:
# LLM_PROVIDER=gemini
# LLM_MODEL=gemini-1.5-flash
# EMBEDDING_MODEL=models/embedding-001
# 
# For Azure OpenAI:
# LLM_PROVIDER=azure_openai
# LLM_MODEL=gpt-4o
# EMBEDDING_MODEL=text-embedding-3-large
# 
# For Ollama (local):
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.1:8b
# EMBEDDING_MODEL=nomic-embed-text
# LLM_API_BASE_URL=http://localhost:11434/v1
# EMBEDDING_API_BASE_URL=http://localhost:11434/v1/embeddings
