# LLM Configuration (Options: openai, gemini, azure_openai, ollama)
LLM_PROVIDER=ollama
LLM_MODEL=gpt-3.5-turbo

# Only set LLM_API_BASE_URL for ollama or custom endpoints
LLM_API_BASE_URL=http://localhost:11434

# Embedding Configuration
# Default embedding model for OpenAI: text-embedding-ada-002
# Default embedding model for Ollama: nomic-embed-text
EMBEDDING_MODEL= nomic-embed-text

# Only set EMBEDDING_API_BASE_URL for ollama or custom endpoints
EMBEDDING_API_BASE_URL=http://localhost:11434

# API Keys
# Replace with your actual API keys for the respective providers
# Ensure to keep these keys secure and do not expose them in public repositories
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
AZURE_OPENAI_API_KEY=your_azure_openai_key
AZURE_OPENAI_ENDPOINT=your_azure_endpoint

# Vector Database Configuration
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION_NAME=knowledge-base-collection

# PostgreSQL Configuration (for pgvector)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=knowledge_base
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password

# GitHub Configuration
GITHUB_TOKEN=your_github_token_here
GITHUB_REPOS=["https://github.com/user/repo1", "https://github.com/user/repo2"]
GITHUB_SUPPORTED_FILE_EXTENSIONS=[".cs", ".csproj", ".sln", ".py", ".sh", ".bash", "dockerfile", ".js", ".jsx", ".ts", ".tsx", ".md", ".txt", ".csv", ".html", ".css", ".json", ".xml", ".yml", ".yaml"]

# Application Configuration
APP_ENV=development
LOG_LEVEL=INFO
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_TOKENS=4000
TEMPERATURE=0.7

# Embedding Batch Processing
EMBEDDING_BATCH_SIZE=50
MAX_TOKENS_PER_BATCH=250000

# Enhanced Chunking
USE_ENHANCED_CHUNKING=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
